{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "\n",
    "trainLabel = pd.read_csv(\"trainLabels.csv\")\n",
    "\n",
    "x_image_paths = [str(i) + \".png\" for i in range(1, 50001)]\n",
    "x_data = []\n",
    "for path in x_image_paths:\n",
    "    x_data.append(imread(\"train/\" + path)) #scipy의 imread로 이미지를 불러들이면 numpy배열로 값을 반환한다.\n",
    "\n",
    "x_data = np.array(x_data) / 256\n",
    "\n",
    "y_data = trainLabel[\"label\"]\n",
    "y_data = y_data.replace([\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"], \n",
    "              [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) #정답 레이블 단어를 숫자 인덱스에 맞도록 치환\n",
    "\n",
    "y_data = y_data.values.reshape((-1, 1))\n",
    "\n",
    "# plt.imshow(x_data[0]) #이미지 출력\n",
    "#x_data[0].shape (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_image_path = [str(i) + \".png\" for i in range(1, 300001)]\n",
    "test_data = []\n",
    "for path in test_image_path:\n",
    "    test_data.append(imread(\"test/\" + path)) #scipy의 imread로 이미지를 불러들이면 numpy배열로 값을 반환한다.\n",
    "\n",
    "test_data = np.array(test_data) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches_train(x_arr, y_arr):\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = 500\n",
    "    n_batches = len(x_arr)//batch_size\n",
    "    \n",
    "    for n in range(0, len(x_arr), batch_size):\n",
    "        # The features\n",
    "        x = x_arr[n:n+batch_size]\n",
    "        # The targets, shifted by one\n",
    "        y = y_arr[n:n+batch_size]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches_test(arr):\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = 500\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    for n in range(0, len(arr), batch_size):\n",
    "        # The features\n",
    "        x = arr[n:n+batch_size]\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3]) #인풋\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1]) #정답레이블\n",
    "keep_prob = tf.placeholder(tf.float32) #드랍아웃 파라미터\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal((5, 5, int(X.get_shape()[-1]), 50), stddev=0.1))\n",
    "b = tf.Variable(tf.random_normal([50]))\n",
    "L = tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding=\"SAME\") #필터 가중치 적용\n",
    "L = tf.nn.bias_add(L, b)\n",
    "L = tf.nn.relu(L)\n",
    "L = tf.nn.max_pool(L, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal((5, 5, int(L.get_shape()[-1]), 128), stddev=0.1))\n",
    "b1 = tf.Variable(tf.random_normal([128]))\n",
    "L1 = tf.nn.conv2d(L, W1, strides=[1, 1, 1, 1], padding=\"SAME\") #필터 가중치 적용\n",
    "L1 = tf.nn.bias_add(L1, b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=0.7) #드랍아웃\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal((5, 5, int(L1.get_shape()[-1]), 256), stddev=0.1))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\") #필터 가중치 적용\n",
    "L2 = tf.nn.bias_add(L2, b2)\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=0.7) #드랍아웃\n",
    "\n",
    "conv = tf.nn.dropout(L2, keep_prob=0.7) #드랍아웃\n",
    "\n",
    "flat = tf.reshape(conv, [-1, int(conv.get_shape()[1]) * int(conv.get_shape()[2]) * int(conv.get_shape()[3])])\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal((int(flat.get_shape()[-1]), 256), stddev=0.1)) #가중치\n",
    "b_fc1 = tf.Variable(tf.random_normal([256])) #편향\n",
    "fc1 = tf.nn.relu(tf.matmul(flat, W_fc1) + b_fc1)\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob=0.7)\n",
    "\n",
    "W_fc2 = tf.Variable(tf.truncated_normal((int(fc1.get_shape()[-1]), 128), stddev=0.1)) #가중치\n",
    "b_fc2 = tf.Variable(tf.random_normal([128])) #편향\n",
    "fc2 = tf.nn.relu(tf.matmul(fc1, W_fc2) + b_fc2)\n",
    "fc2 = tf.nn.dropout(fc2, keep_prob=0.7)\n",
    "\n",
    "W_fc3 = tf.Variable(tf.truncated_normal((int(fc2.get_shape()[-1]), 64), stddev=0.1)) #가중치\n",
    "b_fc3 = tf.Variable(tf.random_normal([64])) #편향\n",
    "fc3 = tf.nn.relu(tf.matmul(fc2, W_fc3) + b_fc3)\n",
    "fc3 = tf.nn.dropout(fc3, keep_prob=0.7)\n",
    "\n",
    "W_fc = tf.Variable(tf.truncated_normal((int(fc3.get_shape()[-1]), 10), stddev=0.1)) #가중치\n",
    "b_fc = tf.Variable(tf.random_normal([10])) #편향\n",
    "\n",
    "out = tf.matmul(fc3, W_fc) + b_fc\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, 10)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, 10])\n",
    "\n",
    "# tf.reset_default_graph() #그래프 초기화\n",
    "\n",
    "logits = tf.identity(out) #결과값\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)) #손실\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost) #Adam으로 최적화\n",
    "\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "prediction = tf.argmax(hypothesis, 1) #예측된 값에서 가장 높은 값을 가지는 요소의 인덱스 추출\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y_one_hot, 1))\n",
    "#결과값의 가장높은 확률(소프트맥스이므로)의 인덱스와 정답레이블의 인덱스 같은 지 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tTraining Step: 50\tLoss: 2.508\n",
      "Step:     0\tTraining Step: 100\tLoss: 2.499\n",
      "Step:     1\tTraining Step: 150\tLoss: 2.357\n",
      "Step:     1\tTraining Step: 200\tLoss: 2.245\n",
      "Step:     2\tTraining Step: 250\tLoss: 2.215\n",
      "Step:     2\tTraining Step: 300\tLoss: 2.161\n",
      "Step:     3\tTraining Step: 350\tLoss: 2.127\n",
      "Step:     3\tTraining Step: 400\tLoss: 2.049\n",
      "Step:     4\tTraining Step: 450\tLoss: 2.020\n",
      "Step:     4\tTraining Step: 500\tLoss: 1.988\n",
      "Step:     5\tTraining Step: 550\tLoss: 1.820\n",
      "Step:     5\tTraining Step: 600\tLoss: 1.866\n",
      "Step:     6\tTraining Step: 650\tLoss: 1.747\n",
      "Step:     6\tTraining Step: 700\tLoss: 1.820\n",
      "Step:     7\tTraining Step: 750\tLoss: 1.680\n",
      "Step:     7\tTraining Step: 800\tLoss: 1.801\n",
      "Step:     8\tTraining Step: 850\tLoss: 1.658\n",
      "Step:     8\tTraining Step: 900\tLoss: 1.662\n",
      "Step:     9\tTraining Step: 950\tLoss: 1.591\n",
      "Step:     9\tTraining Step: 1000\tLoss: 1.580\n",
      "Step:    10\tTraining Step: 1050\tLoss: 1.554\n",
      "Step:    10\tTraining Step: 1100\tLoss: 1.616\n",
      "Step:    11\tTraining Step: 1150\tLoss: 1.501\n",
      "Step:    11\tTraining Step: 1200\tLoss: 1.623\n",
      "Step:    12\tTraining Step: 1250\tLoss: 1.510\n",
      "Step:    12\tTraining Step: 1300\tLoss: 1.542\n",
      "Step:    13\tTraining Step: 1350\tLoss: 1.448\n",
      "Step:    13\tTraining Step: 1400\tLoss: 1.522\n",
      "Step:    14\tTraining Step: 1450\tLoss: 1.431\n",
      "Step:    14\tTraining Step: 1500\tLoss: 1.381\n",
      "Step:    15\tTraining Step: 1550\tLoss: 1.412\n",
      "Step:    15\tTraining Step: 1600\tLoss: 1.407\n",
      "Step:    16\tTraining Step: 1650\tLoss: 1.405\n",
      "Step:    16\tTraining Step: 1700\tLoss: 1.414\n",
      "Step:    17\tTraining Step: 1750\tLoss: 1.324\n",
      "Step:    17\tTraining Step: 1800\tLoss: 1.329\n",
      "Step:    18\tTraining Step: 1850\tLoss: 1.342\n",
      "Step:    18\tTraining Step: 1900\tLoss: 1.338\n",
      "Step:    19\tTraining Step: 1950\tLoss: 1.320\n",
      "Step:    19\tTraining Step: 2000\tLoss: 1.271\n",
      "Step:    20\tTraining Step: 2050\tLoss: 1.267\n",
      "Step:    20\tTraining Step: 2100\tLoss: 1.234\n",
      "Step:    21\tTraining Step: 2150\tLoss: 1.267\n",
      "Step:    21\tTraining Step: 2200\tLoss: 1.204\n",
      "Step:    22\tTraining Step: 2250\tLoss: 1.279\n",
      "Step:    22\tTraining Step: 2300\tLoss: 1.232\n",
      "Step:    23\tTraining Step: 2350\tLoss: 1.212\n",
      "Step:    23\tTraining Step: 2400\tLoss: 1.201\n",
      "Step:    24\tTraining Step: 2450\tLoss: 1.243\n",
      "Step:    24\tTraining Step: 2500\tLoss: 1.304\n",
      "Step:    25\tTraining Step: 2550\tLoss: 1.169\n",
      "Step:    25\tTraining Step: 2600\tLoss: 1.226\n",
      "Step:    26\tTraining Step: 2650\tLoss: 1.185\n",
      "Step:    26\tTraining Step: 2700\tLoss: 1.195\n",
      "Step:    27\tTraining Step: 2750\tLoss: 1.138\n",
      "Step:    27\tTraining Step: 2800\tLoss: 1.165\n",
      "Step:    28\tTraining Step: 2850\tLoss: 1.127\n",
      "Step:    28\tTraining Step: 2900\tLoss: 1.124\n",
      "Step:    29\tTraining Step: 2950\tLoss: 1.087\n",
      "Step:    29\tTraining Step: 3000\tLoss: 1.154\n",
      "Step:    30\tTraining Step: 3050\tLoss: 1.095\n",
      "Step:    30\tTraining Step: 3100\tLoss: 1.043\n",
      "Step:    31\tTraining Step: 3150\tLoss: 1.114\n",
      "Step:    31\tTraining Step: 3200\tLoss: 1.011\n",
      "Step:    32\tTraining Step: 3250\tLoss: 1.099\n",
      "Step:    32\tTraining Step: 3300\tLoss: 1.054\n",
      "Step:    33\tTraining Step: 3350\tLoss: 1.014\n",
      "Step:    33\tTraining Step: 3400\tLoss: 1.030\n",
      "Step:    34\tTraining Step: 3450\tLoss: 1.036\n",
      "Step:    34\tTraining Step: 3500\tLoss: 0.982\n",
      "Step:    35\tTraining Step: 3550\tLoss: 1.002\n",
      "Step:    35\tTraining Step: 3600\tLoss: 0.989\n",
      "Step:    36\tTraining Step: 3650\tLoss: 1.022\n",
      "Step:    36\tTraining Step: 3700\tLoss: 1.038\n",
      "Step:    37\tTraining Step: 3750\tLoss: 1.026\n",
      "Step:    37\tTraining Step: 3800\tLoss: 1.017\n",
      "Step:    38\tTraining Step: 3850\tLoss: 0.973\n",
      "Step:    38\tTraining Step: 3900\tLoss: 0.914\n",
      "Step:    39\tTraining Step: 3950\tLoss: 0.967\n",
      "Step:    39\tTraining Step: 4000\tLoss: 0.917\n",
      "Step:    40\tTraining Step: 4050\tLoss: 0.967\n",
      "Step:    40\tTraining Step: 4100\tLoss: 0.912\n",
      "Step:    41\tTraining Step: 4150\tLoss: 0.945\n",
      "Step:    41\tTraining Step: 4200\tLoss: 0.982\n",
      "Step:    42\tTraining Step: 4250\tLoss: 0.940\n",
      "Step:    42\tTraining Step: 4300\tLoss: 0.853\n",
      "Step:    43\tTraining Step: 4350\tLoss: 0.940\n",
      "Step:    43\tTraining Step: 4400\tLoss: 0.948\n",
      "Step:    44\tTraining Step: 4450\tLoss: 0.840\n",
      "Step:    44\tTraining Step: 4500\tLoss: 0.839\n",
      "Step:    45\tTraining Step: 4550\tLoss: 0.885\n",
      "Step:    45\tTraining Step: 4600\tLoss: 0.854\n",
      "Step:    46\tTraining Step: 4650\tLoss: 0.853\n",
      "Step:    46\tTraining Step: 4700\tLoss: 0.771\n",
      "Step:    47\tTraining Step: 4750\tLoss: 0.788\n",
      "Step:    47\tTraining Step: 4800\tLoss: 0.839\n",
      "Step:    48\tTraining Step: 4850\tLoss: 0.929\n",
      "Step:    48\tTraining Step: 4900\tLoss: 0.792\n",
      "Step:    49\tTraining Step: 4950\tLoss: 0.813\n",
      "Step:    49\tTraining Step: 5000\tLoss: 0.792\n",
      "Step:    50\tTraining Step: 5050\tLoss: 0.821\n",
      "Step:    50\tTraining Step: 5100\tLoss: 0.757\n",
      "Step:    51\tTraining Step: 5150\tLoss: 0.800\n",
      "Step:    51\tTraining Step: 5200\tLoss: 0.770\n",
      "Step:    52\tTraining Step: 5250\tLoss: 0.802\n",
      "Step:    52\tTraining Step: 5300\tLoss: 0.785\n",
      "Step:    53\tTraining Step: 5350\tLoss: 0.837\n",
      "Step:    53\tTraining Step: 5400\tLoss: 0.776\n",
      "Step:    54\tTraining Step: 5450\tLoss: 0.833\n",
      "Step:    54\tTraining Step: 5500\tLoss: 0.796\n",
      "Step:    55\tTraining Step: 5550\tLoss: 0.840\n",
      "Step:    55\tTraining Step: 5600\tLoss: 0.694\n",
      "Step:    56\tTraining Step: 5650\tLoss: 0.782\n",
      "Step:    56\tTraining Step: 5700\tLoss: 0.738\n",
      "Step:    57\tTraining Step: 5750\tLoss: 0.748\n",
      "Step:    57\tTraining Step: 5800\tLoss: 0.739\n",
      "Step:    58\tTraining Step: 5850\tLoss: 0.729\n",
      "Step:    58\tTraining Step: 5900\tLoss: 0.719\n",
      "Step:    59\tTraining Step: 5950\tLoss: 0.731\n",
      "Step:    59\tTraining Step: 6000\tLoss: 0.730\n",
      "Step:    60\tTraining Step: 6050\tLoss: 0.727\n",
      "Step:    60\tTraining Step: 6100\tLoss: 0.727\n",
      "Step:    61\tTraining Step: 6150\tLoss: 0.769\n",
      "Step:    61\tTraining Step: 6200\tLoss: 0.673\n",
      "Step:    62\tTraining Step: 6250\tLoss: 0.731\n",
      "Step:    62\tTraining Step: 6300\tLoss: 0.685\n",
      "Step:    63\tTraining Step: 6350\tLoss: 0.735\n",
      "Step:    63\tTraining Step: 6400\tLoss: 0.642\n",
      "Step:    64\tTraining Step: 6450\tLoss: 0.705\n",
      "Step:    64\tTraining Step: 6500\tLoss: 0.655\n",
      "Step:    65\tTraining Step: 6550\tLoss: 0.707\n",
      "Step:    65\tTraining Step: 6600\tLoss: 0.661\n",
      "Step:    66\tTraining Step: 6650\tLoss: 0.703\n",
      "Step:    66\tTraining Step: 6700\tLoss: 0.633\n",
      "Step:    67\tTraining Step: 6750\tLoss: 0.671\n",
      "Step:    67\tTraining Step: 6800\tLoss: 0.638\n",
      "Step:    68\tTraining Step: 6850\tLoss: 0.711\n",
      "Step:    68\tTraining Step: 6900\tLoss: 0.605\n",
      "Step:    69\tTraining Step: 6950\tLoss: 0.620\n",
      "Step:    69\tTraining Step: 7000\tLoss: 0.643\n",
      "Step:    70\tTraining Step: 7050\tLoss: 0.587\n",
      "Step:    70\tTraining Step: 7100\tLoss: 0.642\n",
      "Step:    71\tTraining Step: 7150\tLoss: 0.628\n",
      "Step:    71\tTraining Step: 7200\tLoss: 0.631\n",
      "Step:    72\tTraining Step: 7250\tLoss: 0.633\n",
      "Step:    72\tTraining Step: 7300\tLoss: 0.594\n",
      "Step:    73\tTraining Step: 7350\tLoss: 0.638\n",
      "Step:    73\tTraining Step: 7400\tLoss: 0.587\n",
      "Step:    74\tTraining Step: 7450\tLoss: 0.664\n",
      "Step:    74\tTraining Step: 7500\tLoss: 0.535\n",
      "Step:    75\tTraining Step: 7550\tLoss: 0.661\n",
      "Step:    75\tTraining Step: 7600\tLoss: 0.581\n",
      "Step:    76\tTraining Step: 7650\tLoss: 0.632\n",
      "Step:    76\tTraining Step: 7700\tLoss: 0.536\n",
      "Step:    77\tTraining Step: 7750\tLoss: 0.580\n",
      "Step:    77\tTraining Step: 7800\tLoss: 0.578\n",
      "Step:    78\tTraining Step: 7850\tLoss: 0.566\n",
      "Step:    78\tTraining Step: 7900\tLoss: 0.497\n",
      "Step:    79\tTraining Step: 7950\tLoss: 0.557\n",
      "Step:    79\tTraining Step: 8000\tLoss: 0.486\n",
      "Step:    80\tTraining Step: 8050\tLoss: 0.592\n",
      "Step:    80\tTraining Step: 8100\tLoss: 0.527\n",
      "Step:    81\tTraining Step: 8150\tLoss: 0.603\n",
      "Step:    81\tTraining Step: 8200\tLoss: 0.509\n",
      "Step:    82\tTraining Step: 8250\tLoss: 0.562\n",
      "Step:    82\tTraining Step: 8300\tLoss: 0.581\n",
      "Step:    83\tTraining Step: 8350\tLoss: 0.598\n",
      "Step:    83\tTraining Step: 8400\tLoss: 0.411\n",
      "Step:    84\tTraining Step: 8450\tLoss: 0.563\n",
      "Step:    84\tTraining Step: 8500\tLoss: 0.523\n",
      "Step:    85\tTraining Step: 8550\tLoss: 0.541\n",
      "Step:    85\tTraining Step: 8600\tLoss: 0.469\n",
      "Step:    86\tTraining Step: 8650\tLoss: 0.519\n",
      "Step:    86\tTraining Step: 8700\tLoss: 0.496\n",
      "Step:    87\tTraining Step: 8750\tLoss: 0.527\n",
      "Step:    87\tTraining Step: 8800\tLoss: 0.503\n",
      "Step:    88\tTraining Step: 8850\tLoss: 0.529\n",
      "Step:    88\tTraining Step: 8900\tLoss: 0.418\n",
      "Step:    89\tTraining Step: 8950\tLoss: 0.509\n",
      "Step:    89\tTraining Step: 9000\tLoss: 0.483\n",
      "Step:    90\tTraining Step: 9050\tLoss: 0.512\n",
      "Step:    90\tTraining Step: 9100\tLoss: 0.469\n",
      "Step:    91\tTraining Step: 9150\tLoss: 0.515\n",
      "Step:    91\tTraining Step: 9200\tLoss: 0.469\n",
      "Step:    92\tTraining Step: 9250\tLoss: 0.556\n",
      "Step:    92\tTraining Step: 9300\tLoss: 0.476\n",
      "Step:    93\tTraining Step: 9350\tLoss: 0.570\n",
      "Step:    93\tTraining Step: 9400\tLoss: 0.455\n",
      "Step:    94\tTraining Step: 9450\tLoss: 0.494\n",
      "Step:    94\tTraining Step: 9500\tLoss: 0.439\n",
      "Step:    95\tTraining Step: 9550\tLoss: 0.450\n",
      "Step:    95\tTraining Step: 9600\tLoss: 0.504\n",
      "Step:    96\tTraining Step: 9650\tLoss: 0.489\n",
      "Step:    96\tTraining Step: 9700\tLoss: 0.415\n",
      "Step:    97\tTraining Step: 9750\tLoss: 0.520\n",
      "Step:    97\tTraining Step: 9800\tLoss: 0.440\n",
      "Step:    98\tTraining Step: 9850\tLoss: 0.418\n",
      "Step:    98\tTraining Step: 9900\tLoss: 0.459\n",
      "Step:    99\tTraining Step: 9950\tLoss: 0.463\n",
      "Step:    99\tTraining Step: 10000\tLoss: 0.432\n"
     ]
    }
   ],
   "source": [
    "pred_arr = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for step in range(100):\n",
    "        for x, y in get_batches_train(x_data, y_data):\n",
    "            counter += 1\n",
    "            session.run(optimizer, feed_dict={X: x, Y: y, keep_prob: 0.7}) #optimizer에 모든 텐서가 연결되어 있으므로 optimizer 실행\n",
    "            loss = session.run(cost, feed_dict={X: x, Y: y, keep_prob: 0.7})\n",
    "            if counter%50 == 0:\n",
    "                print(\"Step: {:5}\\tTraining Step: {}\\tLoss: {:.3f}\".format(step, counter, loss))\n",
    "\n",
    "    for x in get_batches_test(test_data):\n",
    "        pred = session.run(prediction, feed_dict={X: x, keep_prob: 1}) \n",
    "        pred_arr.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = pd.Series(pred_arr) \n",
    "result = result.replace([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                        [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]) #정답 레이블 단어를 숫자 인덱스에 맞도록 치환\n",
    "result.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result.to_csv(r'pandas.csv', header=None, index=True, sep=',', mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
